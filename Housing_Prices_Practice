{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":10695,"sourceType":"datasetVersion","datasetId":7519},{"sourceId":8917134,"sourceType":"datasetVersion","datasetId":5362767}],"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\nfor dirname, _, filenames in os.walk('/kaggle/working'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\n\ndf = pd.read_csv(\"/kaggle/input/rb-titanic/train.csv\")\nprint(df)\nprint(df.info())\nprint(\"\\nDescriptive statistics:\")\nprint(df.describe())\n\n# Check for missing values\nprint(\"\\nMissing values:\")\nprint(df.isnull().sum())\n","metadata":{"execution":{"iopub.status.busy":"2024-07-21T16:08:47.5882Z","iopub.execute_input":"2024-07-21T16:08:47.588604Z","iopub.status.idle":"2024-07-21T16:08:47.642841Z","shell.execute_reply.started":"2024-07-21T16:08:47.588572Z","shell.execute_reply":"2024-07-21T16:08:47.641423Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nfrom sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\nfrom sklearn.metrics import mean_squared_error, accuracy_score\n#import polars as pl\nimport pandas as pd\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder, MinMaxScaler, FunctionTransformer, PowerTransformer\nfrom sklearn.decomposition import PCA\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import r2_score\n\n# Notes: PCA didn't help!?\n####\n# NEXT STEPS: internally split the data into a sub-test so we can iterate faster!\n# MAKE SUBMISSION A SECOND SECTION?\n\nversion = 12\nname = \"pca_95_sibs_parents\"\n\nFINAL_TEST = True\nPCA_PERCENT = 95  # EXPERIMENTS = range(50, 76, 1)\nif FINAL_TEST:\n    EXPERIMENTS = [PCA_PERCENT] if FINAL_TEST else EXPERIMENTS\nelse:\n    EXPERIMENTS = range(50, 99, 2)\n\n#DROP_FEATURES = [\"PassengerId\", \"Survived\", \"Pclass\", \"Name\", \"Ticket\", \"Fare\", \"Cabin\"]\nLABEL = [\"Survived\"]\nSCALED_FEATURES = [\"Age\", \"SibSp\", \"Parch\"]  # Fare did not seem to improve\nCAT_FEATURES = [\"Embarked\", \"Pclass\"]\nBOOL_FEATURES = [\"Sex\"]\n\ndef make_pipeline(pca_percent=PCA_PERCENT):\n    print(pca_percent)\n    # Create the preprocessing steps for specific column processing\n    preprocessor = ColumnTransformer(\n        transformers=[\n            (\n                'num',\n                Pipeline(steps=\n                    [\n                        ('average', SimpleImputer(missing_values=np.nan, strategy='mean')),\n                        ('scale', StandardScaler()),\n                    ],\n                ),\n                 SCALED_FEATURES,\n            ),\n            (\n                'cat',\n                Pipeline(steps=\n                    [\n                        ('imputer', SimpleImputer(strategy='most_frequent')),\n                        ('onehot', OneHotEncoder(handle_unknown='ignore', drop=\"first\", sparse_output=False))\n                    ],\n                ),\n                CAT_FEATURES + BOOL_FEATURES,\n            ),\n        ],\n        remainder=\"drop\",\n    )\n\n    # Create the full pipeline\n    pipeline = Pipeline([\n        ('preprocess', preprocessor),\n        ('pca', PCA(n_components=pca_percent/100)),\n        ('classifier', RandomForestClassifier(n_estimators=100, random_state=42)),\n    ])\n    \n    return pipeline\n\n# Get train/test features and labels\ndf = pd.read_csv(\"/kaggle/input/rb-titanic/train.csv\")\nX = df.drop(LABEL, axis=1)\ny = df[LABEL]\ny = y.values.ravel()\nif FINAL_TEST:\n    X_train = X\n    y_train = y\n    X_test = pd.read_csv(\"/kaggle/input/rb-titanic/test.csv\")\n    # NOTE: There is no y_test in the final\nelse:\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=123)\n\nfor experiment in EXPERIMENTS:\n    # Train with pipeline\n    pipeline = make_pipeline(experiment)\n    pipeline.fit(X_train, y_train)\n\n    # Make predictions on the test set\n    y_pred = pipeline.predict(X_test)\n\n    # Evaluate the model\n    if not FINAL_TEST:\n        print(f\"{version}_{name} SCORES\")\n        mse = mean_squared_error(y_test, y_pred)  # <-- NO y_test/Survived IN DATA. Submit?\n        rmse = np.sqrt(mse)\n        print(f\"RMSE:\\t{rmse}\")\n        r2 = r2_score(y_test, y_pred)\n        print(f\"R2:\\t{r2}\")\n        accuracy = accuracy_score(y_test, y_pred)\n        print(f\"Acc:\\t{accuracy}\")\n        from sklearn.metrics import f1_score\n        f1 = f1_score(y_test, y_pred)\n        print(f\"F1:\\t{f1}\")\n        print(\"---\")\n\n    # Save the final test results\n    else:\n        # Save the DataFrame to a CSV file\n        results_df = pd.DataFrame({\n            \"PassengerId\": X_test[\"PassengerId\"],\n            \"Survived\": y_pred,\n        })\n        print(results_df)\n\n        csv_filename = f\"titanic_restart_{version}_{name}.csv\"\n        results_df.to_csv(csv_filename, index=False)\n        print(f\"Predictions saved to {csv_filename}\")\n","metadata":{"execution":{"iopub.status.busy":"2024-07-21T16:22:16.33291Z","iopub.execute_input":"2024-07-21T16:22:16.333426Z","iopub.status.idle":"2024-07-21T16:22:16.806592Z","shell.execute_reply.started":"2024-07-21T16:22:16.333389Z","shell.execute_reply":"2024-07-21T16:22:16.805068Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}